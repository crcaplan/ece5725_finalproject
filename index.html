
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>ECE 5725: Touchless Music Player</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

  </head>

  <body style="background-color:whitesmoke;">

    <nav class="navbar navbar-inverse navbar-fixed-top" style="background-color: #0a6194">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Touchless Music Player</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="#">Home</a></li>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#obj">Objective</a></li>
            <li><a href="#design">Design</a></li>
            <li><a href="#testings">Testings</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#future">Future Works</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div class="starter-template">
        <h1>Touchless Music Player</h1>
        <p class="lead">ECE 5725 Project<br>Tamzid Ahmed (ta326) and Claire Caplan (crc235)<br>05/19/2021</p>
      </div>

      <hr>
      <div class="center-block">
          <h2 style="text-align:center;">Demonstration Video</h2>
          <iframe width="640" height="360" src="https://www.cornell.edu/video/glorious-to-view" frameborder="0" allowfullscreen></iframe>
      </div>

      <hr id='obj'>

    <div style="text-align:center;">
        <h2>Objective</h2>
        <p style="text-align: left;padding: 0px 30px;">Touchless music player uses a Raspberry Pi to function as a jukebox by allowing users to make their own personalized playlist and select a song to be played from the playlist on a speaker connected to the Raspberry Pi. Each user has their own QR code that they need to scan so that their playlist can be displayed on piTFT from which they can select the song that they want to play through hand gestures</p>
    </div>

    <hr id="intro">
    <div style="text-align:center;">
      <h2>Introduction</h2>
      <p style="text-align: left;padding: 0px 30px;"> The project started with installing opencv and required libraries on Raspberry Pi. We installed piCamera and integrated it with openCV to detect QR code when the user holds the QRcode in front of the camera. We wrote a python script to detect the QRcode using openCV and qrcode library from python. Next we collected all the required parts needed for the hand gesture recognition circuit. We used Adafruit break beam sensors for song selection, play/pause and an ultrasonic sensor to control the volume. The explanation of how the circuit works will be explained in the design section. Once we made a schematic for the hand gesture and checked the correctness of the circuit with the TAs we tested the circuit by checking the piscope signal and writing python scripts and checking the outputs in the terminal. After that we wrote a script to control song selection using IR sensors and volume control using ultrasonic sensors. Once the song selection was working, we had to integrate the QR scanning with song selection so that once the users scan their QR code, they can play music using hand gestures. We also added an option for a new user to receive a QR code and select the songs that they want to be in their playlist. Once all the programming was done, we also made a 3D printed structure to hold the whole music system. Our device, as well as both team members, can be seen in Figure 1 below. </p>
      <br>
      <img class="img-rounded" src="" alt="Figure 1: Finished “Touchless Music Player” prototype" width="600" style="text-align: center">
    </div>
    <!--
    <div style="text-align:center;">
            <h2>Introduction</h2>
            <h4 style="text-align: left;text-indent: 0.8cm">Motivation: </h4>
            <p style="text-align: left;padding: 0px 30px;">We are first inspired on how drones are able to track the user as they record a video, providing a convenient experience for the user. We then thought about having a ground robot that has a similar tracking capability, where it will be able to carry things around and also operate indoors. The user will be able to seamlessly interact with the robot using finger gestures. This robot can be installed on shopping carts in grocery stores or luggage carts in the airport. Having this robot will allow the user to conserve their energy (from carrying items or pushing carts) and also focus their attention on other things, such as looking for the things they wanted to get from the grocery racks.</p>
            <h4 style="text-align: left;text-indent: 0.8cm">Solution: </h4>
            <p style="text-align: left;padding: 0px 30px;">A robot is designed to be able to track and follow a ball (representing a human), as well as to read hand-gesture inputs from the user. Computer Vision is utilized to process the video input the camera to detect for the ball and hand gestures. PID controller mechanism is used for the robot motor for smooth tracking.</p>
    </div>
    -->
    

    <hr id='design'>

      <div style="text-align:center;">
              <h2>Design</h2>            
            <p style="text-align: left;padding: 0px 30px;">
              <br>In the section below, we will have a more in-depth discussion on how each subsystem is designed.
            </p>
              <h4 style="text-align: left;text-indent: 0.8cm">QR Code Detection: </h4>
              <p style="text-align: left;padding: 0px 30px;"> 
                We began our project by first installing OpenCV on the Raspberry Pi. Following the instructions provided on Canvas, we were able to install OpenCV for Python3. Using a camera installation reference video from the Raspberry Pi official channel, we installed the PiCamera on the Raspberry Pi. Next we used the provided Canvas example on OpenCV to learn how to use OpenCV and to install the required libraries needed for OpenCV and QR code scanners [1]. Following the instructions in [1], we were able to write a python script qr_scanner.py that can scan a QR code using the PiCamera and show the message embedded in the QR code on the PiCamera window and the terminal. We tested the script by checking if the previously generated QR codes can be detected and if the message embedded in the QR code can be read by the camera. Once the script reads the data from the QR code, it checks if the username already exists in a text file called netid.txt where we stored all known users in the system.
                <br>
                <br>
                All the testing early in the project development was done on the desktop, where the Pygame GUI and camera feed were in two separate windows like in Figure 3 below.
            </p>
            <img class="img-rounded" src="" alt="Figure 2: Desktop testing of camera scanner code" width="600" style="text-align: center">
            <br>
            <br>
            <p style="text-align: left;padding: 0px 30px;">
              We wanted our project to be deployed on a standalone Raspberry Pi and thus we needed the camera window to show up on the PiTFT display. Initially we could not get the camera window to show up on the PiTFT display as our program would just hang and the PiTFT would not respond to any further inputs. After following references from the Air Canvas project from Fall 2019 we were able to make the camera window show up on PiTFT. We needed to rescale the PiCamera feed window to PiTFT screen size(320x240) and render the camera display on the Pygame window, which we implemented in a helper function, and then the camera feed showed up as expected. This is seen below in Figure 3.
              <br>
              <br>
          </p>
          <img class="img-rounded" src="" alt="Figure 3: PiTFT testing of camera scanner code" width="600" style="text-align: center">
          <br>
          <br>
              <h4 style="text-align: left;text-indent: 0.8cm">Sensor schematic development: </h4>
              <br>
              <br>
              <p style="text-align: left;padding: 0px 30px;">
                Initially, we planned to use a combination of IR emitters and photodiodes to sense hand motions for play/pause control and skipping/rewinding tracks, and an ultrasonic sensor to control volume based on hand distance from the music playing unit. However, the IR emitter/photodiode pairs would have required an unnecessarily complicated hardware setup involving a tunable amplifier circuit with an op-amp and potentiometers in order to be read by the Raspberry Pi GPIO pins. At the suggestion of the professor, we elected to design our hand motion detection hardware around two emitter/receiver pairs of Adafruit break-beam sensors, which simplified this aspect of the hardware greatly and increased its reliability.
                <br>
                <br>
             The break-beam sensors come in emitter/receiver pairs. The emitter is connected to a voltage source and grounded, and emits a constant IR beam. The receiver is an IR-sensitive device, and is also connected to the same voltage source and ground, but it includes an additional signal output. When the beam “connection” is broken by a non IR-transparent object (for example, the human hand), this signal output will drop to a logic low which can be detected by a Raspberry Pi GPIO input-configured pin.
                <br>
                <br>
             The ultrasonic sensor estimates an object’s distance from its emitters by sending out ultrasonic sound pulses and converting the reflected sound into electrical signals. Our sensor is powered by a 5V supply from the Raspberry Pi, and grounded on the same common ground as the rest of our circuit. From a GPIO output pin, we send electrical pulse signals over the trigger pin to trigger ultrasonic emissions from the device. We read the reflected electrical signal on a GPIO input pin and determine distance from the pulse width in time of the reflected signal.
                <br>
                <br>
               Because we chose to run both the break-beam sensors and the ultrasonic sensor off of a 5V supply from the Raspberry Pi, we needed to step the voltage outputs of these sensors down to 3.3V because this is the maximum logic voltage supported by the Raspberry Pi’s GPIO pins. We initially set up our ultrasonic sensor with a voltage divider which halved the 5V output of the sensor to a maximum of 2.5V. Our initial schematic with this implementation is shown below in Figure 4
            </p>
            <img class="img-rounded" src="" alt="Figure 4 PiTFT testing of camera scanner code" width="600" style="text-align: center">
            <br>
            <br>
            <p style="text-align: left;padding: 0px 30px;">
              We believed that this would work as intended because the Raspberry Pi’s logic level threshold is 1.8V, but in our initial tests of the ultrasonic sensor we were not receiving reliable distance readings (which would have translated to spotty volume control). One possible solution would have been to implement voltage dividers that resulted in a 3.3V output, but this would require many resistors and an overly complicated breadboarding process, since we also needed to step down the logic signals from both break-beam receivers. Since all of these signals were digital (hence being read by Raspberry Pi GPIO pins), we did not need to preserve any analog components of the signals, and could implement a simpler logic voltage translation using a 5V to 3.3V level shifter from Sparkfun. This level shifter operates based on two “high” and “low” reference voltages (in our case, 5V and 3.3V respectively, both of which were provided by the Raspberry Pi), and translates up to 4 “high” voltage inputs down to “low” voltages via a network of MOSFETs. We sent both of the break-beam receiver signals through this level shifter, as well as the echo pin of the ultrasonic sensor. At the low side of the level shifter, each of the outputs is sent to a GPIO input-configured pin, with a 1k resistor included as well for current protection. The final schematic can be seen in Figure 5, and the Fritzing project files can be downloaded here.
          </p>
          <br>
          <img class="img-rounded" src="" alt="Figure 5: Final sensor hardware schematic" width="600" style="text-align: center">
          <br>
          <br>
          <h4 style="text-align: left;text-indent: 0.8cm">Sensor Testing: </h4>
          <br>
          <br>
          <p style="text-align: left;padding: 0px 30px;">
          We tested basic functionality of the break-beam sensors by connecting them as shown/described in the schematic section above, with the receiver output signals connected to GPIO input pins. To observe that they were functioning properly, we ran PiScope as we were instructed to in previous labs in the course. By observing the signals on the two GPIO pins that were connected to receiver outputs, we were able to see that the signals went to logic low when the beam “connections” were broken between sensor pairs, and showed logic high otherwise. These results are shown in the PiScope screenshots below in Figure 6. </p>
          <br>
          <img class="img-rounded" src="" alt="Figure 6 PiScope testing of break-beam sensors" width="600" style="text-align: center">
          <br>
          <br>
          <p style="text-align: left;padding: 0px 30px;">
              We tested basic distance reading functionality of the ultrasonic sensor by writing a short script called distance.py. This script calculates the distance by sending a trigger pulse and the received pulse’s width in time on the echo line. We begin by setting the trigger GPIO output to low for 0.5 seconds to allow the sensor to settle from the last reflected pulse. We then send a pulse out on the trigger pin by setting the GPIO output to high for 0.00001 seconds. Finally, we take a timestamp at the beginning and end of the echo pulse, based on the times that the GPIO input reads logic 0 and 1. Finally, we convert this pulse width to a distance in centimeters using a conversion factor found from the Adafruit site. As can be seen in Figure 8 below, we were able to get fairly accurate readings of around 10cm, with about 8% error from the expected value of 10 that we measured with a tape measure.
          </p>
          <img class="img-rounded" src="" alt="Figure 8: Ultrasonic sensor testing with ‘distance.py’" width="600" style="text-align: center">
          <img class="img-rounded" src="" alt="Figure 8: Ultrasonic sensor testing with ‘distance.py’" width="600" style="text-align: center">
          <br>
<br>
<h4 style="text-align: left;text-indent: 0.8cm">Music playback: OMXPlayer: </h4>
<br>
<br>
<p style="text-align: left;padding: 0px 30px;">
    We chose to create an internal song library in our project which is stored locally on the Raspberry Pi. We downloaded mp3 files to put in our library using a YouTube to mp3 conversion site. Initially we wanted to use mplayer to play the music, however when we played music on mplayer opened on a subprocess, it was not responding to the commands that we were sending through subprocess input. After searching we found one project using OMXPlayer for playing music and when we tried OMXPlayer, it was responsive to the commands sent [2]. Thus we decided to use OMXPlayer for the project. The list below describes the OMXPlayer commands that we used in our project to control song playback.
    <br>
    <br>
command                  Function
q                                exit omxplayer/stop the music playing
p                                pause/play music
+                                Increase volume
-	                      decrease volume
 <br>
 <br>
</p>
<h4 style="text-align: left;text-indent: 0.8cm">Software integration: sensor code for music control: </h4>
<br>
<br>
<p style="text-align: left;padding: 0px 30px;">
    sensor.py to control music playback. We have a folder called song_library in our project that contains the local song library of all the songs that we downloaded from YouTube, which the users can then select from to make their playlist. We used the glob library from Python to keep the list of all the paths of the songs from the song_library folder in sensor.py. A pointer was used to iterate through the list of the songs in the local library. The songs were played by opening OMXPlayer through a subprocess (just as we did with mplayer in our labs) using the pointer to choose a specific song from the song list, and sending commands to the player through subprocess stdin. 
    <br>
    <br>
    To read from all of the sensors and handle appropriate song playback control, we placed a series of if-else statements inside of a while loop to continuously poll the sensors while leaving room to integrate easily with a Pygame GUI later in the design process.
    <br>
    <br>
   We positioned two emitters and receivers in such a way so that each receiver can receive the signal from a corresponding emitter without interference from one another. The functionalities are described below:
    <br>
    <br>
    i) Play/Pause: Both receivers blocked by hands
    <br>
ii) Next song: Right receiver blocked
    <br>
iii) Previous song: Left receiver blocked
<br>
    <br>
    Since the emitter and receiver are facing each other, the receiver continuously receives a high signal until the emitter is blocked. Thus we used if else statements to check if GPIO pins connected to receivers are receiving 0 and take appropriate action based on which combinations of receivers are blocked. If the GPIO pin connected to the right receiver gets a ‘0’, the program sends a ‘q’ input signal to the OMXPlayer which stops the player. Next the pointer is incremented by 1 and the player is started again with the next song from the list. Similarly, if the  GPIO pin connected to the left receiver gets a ‘0’, the pointer is decremented and the previous song is selected by quitting and restarting OMXPlayer. We also handle the edge cases when the pointer reaches the end of the list and the beginning of the list by rolling over the pointer to keep it from getting out of range.
    <br>
    <br>
    In order to control volume, we used the function from the ‘distance.py’ testing script to get a distance in centimeters of the user’s hand from the ultrasonic sensor’s base. We called this function after checking both break-beam sensors in the if statements described above, to get the distance of the user’s hand from the sensor. Then, we included an if statement to check if the recorded distance was between 1 and 10 centimeters (so that the sensor doesn’t trigger volume changes from a user’s unintentional motions) and if this was the case we compared it to the current volume and iterated through a for loop to increment or decrement the volume in both our code counter and omxplayer (using the ‘+’ command) until it reached the same number as the distance sensor reading.
</p>
<br>
<br>
<h4 style="text-align: left;text-indent: 0.8cm">Software integration: Pygame GUI: </h4>
<br>
<p style="text-align: left;padding: 0px 30px;">
  In order to make device usage clear and concise to our users, we created a Pygame GUI framework. The first screen in the GUI is the start screen, which has two buttons: ‘Start’ and ‘Quit.’ The ‘Start’ button takes the users to the next screen and the ‘Quit’ button ends the program. We also had a welcome message in the start screen. Below is a picture of the start screen
</p>
<img class="img-rounded" src="" alt="Figure 9: Start screen in Pygame GUI" width="600" style="text-align: center">
<br>
<br>
<p style="text-align: left;padding: 0px 30px;">
  The next two screens are the instruction screens that instruct the user to 1) scan their QR code to load their personal playlist, and 2) describe how the touchless control sensors work. The first instruction screen contains the same ‘Quit’ button as the start screen, as well as a ‘Next’ button to proceed to the next instruction page. In addition, this screen also has a new user button which a new user needs to press to get their QR code and make their playlist. Pre-existing users can press the next button twice to proceed through the sensor control instructions to the scan screen.
  </p>
<img class="img-rounded" src="" alt="Figure 10: First instruction screen in Pygame GUI" width="600" style="text-align: center">
<br>
<br>
<p style="text-align: left;padding: 0px 30px;">
  If a new user wants to create a custom playlist, they must click on the new user button on the instruction screen which will generate a new user id, QR code, and a playlist folder for the user. The user id is saved in the ‘netid.txt’ file to be used later in scanning. The user is also instructed to take a picture of their QR code for future use.
</p>
<img class="img-rounded" src="" alt="Figure 11: New user QR code display" width="600" style="text-align: center">
<br>
<br>
<p style="text-align: left;padding: 0px 30px;">
  Once the user saves their QR code and presses the ‘Song Selection’ button, the songs from the song_library folder are shown with each song’s information and album cover image on one page. The user can then use ‘Prev’ and ‘Next’ buttons to traverse through the songs. Pressing on the ‘Select’ button adds the song to the user’s playlist which was created in the last step. Once the user is done selecting the songs they can press the ‘Done’ button which redirects the user to the first instruction screen.
</p>
<img class="img-rounded" src="" alt="Figure 12: Figure 12: New user playlist selection interface" width="600" style="text-align: center">
<br>
<br>
<p style="text-align: left;padding: 0px 30px;">
  Any user who has a pre-existing saved QR code and playlist only needs to press the ‘Next’ button from the instruction screen which will show them one more instruction screen with information on the touchless control sensors. Pressing ‘Next’ again shows the scan initiation screen. These two screens are shown below
</p>
<img class="img-rounded" src="" alt="Figure 13: Second instruction screen and scan initiation screen" width="600" style="text-align: center">
<img class="img-rounded" src="" alt="Figure 13: Second instruction screen and scan initiation screen" width="600" style="text-align: center">
<br>
<br>
<p style="text-align: left;padding: 0px 30px;">
  Pressing on the green ‘Scan’ button on the scan screen displays the camera window on the PiTFT screen which the users can then use to orient their QR code for scanning.
  Once the scan is successful the user is redirected to their already existing playlist and from there the user can use hand gestures to play/pause, change songs, and increase/decrease volume. There is also a ‘Quit’ button in the GUI which takes the users back to the initial start screen once they are finished playing songs. The music playing user interface is shown below.
</p>
<img class="img-rounded" src="" alt="Figure 14: Music Playing Interface" width="600" style="text-align: center">
<br>
<br>
<h4 style="text-align: left;text-indent: 0.8cm">Mounting scheme: CAD and 3D printing: </h4>
<br>
<p style="text-align: left;padding: 0px 30px;">
In designing our mounting enclosure for the hardware, we began by listing the needs of our electrical system. First was our primary motivation for the mechanical component: a mounting scheme for the break-beam sensors that would align both pairs and raise them to a height that was suitable for controlling with hand gestures. Second, we also wanted a cover to contain all of the wiring and breadboard, so that it would be out of sight of the user as well as protected from any accidental motion or snags that could disrupt its function. We also decided that we wanted a cutout to place the ultrasonic sensor in the center of the mounting enclosure. Finally, we decided that we wanted a stand for the Raspberry Pi to act as a display, as well as a place to mount the PiCamera.
  <br>
  <br>
  After determining the needs of our mechanical system, we sketched out a rough idea of what we wanted the prototype to look like. Our preliminary design sketch can be found in Figure 15.
</p>
<img class="img-rounded" src="" alt="Figure 15: Initial sketch of mounting enclosure design" width="600" style="text-align: center">
<br>
<p style="text-align: left;padding: 0px 30px;">
  We also measured all the electrical parts with calipers to ensure exact fits in our prototype, including the break-beam sensors (and their mounting hole diameters), the ultrasonic sensor, the breadboard, and the Raspberry Pi case. Based on our initial measurements and rough sketch, we rendered our design in CAD using Autodesk Inventor. Different angles of our prototype in CAD can be seen in Figure 16 below.
</p>
<img class="img-rounded" src="" alt="Figure 16: Mounting enclosure inCAD" width="600" style="text-align: center">
<br>
<p style="text-align: left;padding: 0px 30px;">
  Finally, we exported the design as a STL file to be 3D printed. However, we ran into some issues with the first iteration of our part, as it was extremely difficult to orient and slice on a 3D printer and used a ton of supports which wastes a lot of material and time. The first iteration rendered in the printer software can be seen in Figure 17 below, with the significant amount of support material highlighted in green. 
</p>
<img class="img-rounded" src="" alt="Figure 17: First iteration printing specs and 3D rendering" width="600" style="text-align: center">
<br>
<p style="text-align: left;padding: 0px 30px;">
 To address the printing issues, we chose to split our mounting enclosure into two separate pieces - one for the Raspberry Pi stand, and one for the breadboard cover and sensor mounts. This significantly improved printing time, and reduced the amount of wasted material on supports. The second iteration, with more efficient supports shown in green, can be seen in Figure 18. Note the printing specs again on the right side, which indicate a lower material usage and cut a significant amount of time off of the printing process. The final printed parts can be seen in Figure 19. Once both parts were printed, we assembled the full mounting enclosure by hot gluing them together. 
</p>
<img class="img-rounded" src="" alt="Figure 18:  Second iteration printing specs and 3D rendering" width="600" style="text-align: center">
<img class="img-rounded" src="" alt="Figure 19: Final printed parts" width="600" style="text-align: center">
<br>
<br>
<h4 style="text-align: left;text-indent: 0.8cm">Final product assembly: mechanical and electrical integration: </h4>
<br>
<br>
<p style="text-align: left;padding: 0px 30px;">
 Once the 3D printed mounting enclosure was assembled, we mounted our sensors and the Raspberry Pi. The break-beam sensors were assembled on the mounting posts using M1.6 screws. However, we were not able to find M1.6 locknuts, so we secured the screws by placing a drop of hot glue on the back of each 3D printed post. We also taped the ultrasonic sensor to the edges of the enclosure cutout using electrical tape. Finally, to reach the breadboard underneath the enclosure, we spliced jumper wires to the ends of the sensor wires using solder-and-seal heat shrink connectors. Images of the installation process can be seen in Figure 20. 
</p>
<img class="img-rounded" src="" alt="Figure 20: Sensor installation pictures" width="600" style="text-align: center">
<img class="img-rounded" src="" alt="Figure 20: Sensor installation picturess" width="600" style="text-align: center">
<img class="img-rounded" src="" alt="Figure 20: Sensor installation pictures" width="600" style="text-align: center">

<br>
<p style="text-align: left;padding: 0px 30px;">
 Finally, we connected the Raspberry Pi to our setup by sending the PiCobbler breakout cable through the designated cutout in the enclosure. However, when we measured the hardware setup with calipers, we failed to account for the power and AUX connection ports.- ideally, the ledge that holds the Raspberry Pi in our structure would have cutouts for these ports so that we could stand it up vertically. To leave room for the ports, we sat the Raspberry Pi horizontally on the stand, and mounted the PiCamera to the vertical part of the stand.
</p>

      </div>

    <hr id='testings'>

      <div style="text-align:center;">
              <h2>Testings</h2>
              <h4 style="text-align: left;text-indent: 0.8cm">Ball tracking system: </h4>
              <p style="text-align: left;padding: 0px 30px;">
              </p>

              <h4 style="text-align: left;text-indent: 0.8cm">Hand gesture detection: </h4>
              <p style="text-align: left;padding: 0px 30px;"> </p>

      </div>
      <div style="text-align:left;">        
        <pre><code>
            inser Generic code</code></pre>
      </div>
      <div style="text-align:center;">
              <p style="text-align: left;padding: 0px 30px;">
              </p>
              <h4 style="text-align: left;text-indent: 0.8cm">Motor Control: </h4>
              <p style="text-align: left;padding: 0px 30px;"></p>
              <br> Several images describing how our robot system was tested.<br><br>
        </div>
        <div style="text-align:left;">        
                <pre><code>
                # genereic code</code></pre>
            
            
            <h4 style="text-align: left;text-indent: 0.8cm">User Interface: </h4>
            <p style="text-align: left;padding: 0px 30px;"></p>
            
            
      </div>
      <div style="text-align: center">
        <img class="img-rounded" src="" alt="Generic placeholder image" style="width:30%;">
        <img class="img-rounded" src="" alt="Generic placeholder image" style="width:30%;">
    </div>
      
      
            

    <hr id='results'>

      <div style="text-align:center;">
              <h2>Results</h2>
              <p style="text-align: left;padding: 0px 30px;"></p>

      </div>
    
    
      <hr id='future'>

      <div style="text-align:center;">
              <h2>Future Works</h2>
              <p style="text-align: left;padding: 0px 30px;"> </p>
      </div>

      <hr id='conclusion'>

      <div style="text-align:center;">
              <h2>Conclusion</h2>
      </div>

    <hr>

    <div style="font-size:18px; text-align: center">
      <h2>Parts List</h2>
      <ul style="text-align: left; text-indent: 10cm">
          <p>- Raspberry Pi 4 $35.00</p>
          <a href="https://www.adafruit.com/product/1601"><p>- Adafruit PiTFT $35.00</p></a>
          <a href="https://www.raspberrypi.org/products/camera-module-v2/"><p>- Raspberry Pi Camera Module V2 $25.00</p></a>
          <p>- Robot base with DC motors $25.00</p>
          <p>- LEDs, Resistors and Wires - Provided during the course</p>
      </ul>
      <h4 style="text-align: center;">Total: $95.00</h4>
  </div>

    

    <hr>
    
    <div class="row" style="text-align:center;">
      <h2>Work Distribution</h2>
      <div class="col-md-6" style="font-size:16px">
          <img class="img-rounded" src="" alt="Generic placeholder image" width="240" height="240">
          <h3>Tamzid Ahmed</h3>
          <p class="lead">ta326@cornell.edu<br>ECE '21</p>
          <div style="text-align:left">
            <li>some work
          </div>
      </div>
      <div class="col-md-6" style="font-size:16px">
          <img class="img-rounded" src="" alt="Generic placeholder image" width="240" height="240">
          <h3>Claire Caplan</h3>
          <p class="lead">crc236@cornell.edu<br>ECE '21</p>
          <div style="text-align:left">
                <li>
          </div>
      </div>
  </div>

      <hr>
      <div style="font-size:18px">
          <h2>References</h2>
          <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
          <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
          <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>
          <a href="http://creat-tabu.blogspot.com/2013/08/opencv-python-hand-gesture-recognition.html">Hand gesture computer vision tutorial</a><br>
          <a href="https://courses.ece.cornell.edu/ece5990/ECE5725_Spring2020_Projects/May_15_Demo/Air%20Painter/awt46_sc2524_Thursday-3/awt46_sc2524_Thursday/index.html">ECE 5725 Air Painter project</a><br>
          <a href="https://courses.ece.cornell.edu/ece5990/ECE5725_Fall2018_Projects/ty359_yw996_Final_Project/index.html">ECE 5725 Tracking Robotic Car project</a><br>

      </div>

    <hr>

      <div class="row" style="font-size:18px">
              <h2>Code Appendix</h2>
              
              <a href="https://github.com/JonathanNusantara/BallTrackingRobot">Github Repository</a><br><br>
              <pre><code>
              </code></pre>
      </div>

    </div><!-- /.container -->




    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>
