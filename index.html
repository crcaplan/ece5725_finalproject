
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>ECE 5725: Touchless Music Player</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

  </head>

  <body style="background-color:whitesmoke;">

    <nav class="navbar navbar-inverse navbar-fixed-top" style="background-color: #0a6194">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Touchless Music Player</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="#">Home</a></li>
            <li><a href="#obj">Objective</a></li>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#design">Design & Testing</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#future">Future Works</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div class="starter-template">
        <h1>Touchless Music Player</h1>
        <p class="lead">ECE 5725 Project<br>Tamzid Ahmed (ta326) and Claire Caplan (crc235)<br>05/19/2021</p>
      </div>

      <hr>
      <div class="center-block">
          <h2 style="text-align:center;">Demonstration Video</h2>
          <iframe width="640" height="360" src="https://www.cornell.edu/video/glorious-to-view" frameborder="0" allowfullscreen></iframe>
      </div>

    <hr id='obj'>
    <div style="text-align:center;">
        <h2>Objective</h2>
        <p style="text-align: left;padding: 0px 30px;">Touchless music player uses a Raspberry Pi to function as a jukebox by allowing users to make their own personalized playlist and select a song to be played from the playlist on a speaker connected to the Raspberry Pi. Each user has their own QR code that they need to scan so that their playlist can be displayed on piTFT from which they can select the song that they want to play through hand gestures</p>
          <!--<img class="img-rounded" src="" alt="picture of the 3d structure" style="width:30%;">-->
      </div>

    <hr id="intro">
    <div style="text-align:center;">
      <h2>Introduction</h2>
      <p style="text-align: left;padding: 0px 30px;"> The project started with installing opencv and required libraries on Raspberry Pi. We installed piCamera and integrated it with openCV to detect QR code when the user holds the QRcode in front of the camera. We wrote a python script to detect the QRcode using openCV and qrcode library from python. Next we collected all the required parts needed for the hand gesture recognition circuit. We used Adafruit break beam sensors for song selection, play/pause and an ultrasonic sensor to control the volume. The explanation of how the circuit works will be explained in the design section. Once we made a schematic for the hand gesture and checked the correctness of the circuit with the TAs we tested the circuit by checking the piscope signal and writing python scripts and checking the outputs in the terminal. After that we wrote a script to control song selection using IR sensors and volume control using ultrasonic sensors. Once the song selection was working, we had to integrate the QR scanning with song selection so that once the users scan their QR code, they can play music using hand gestures. We also added an option for a new user to receive a QR code and select the songs that they want to be in their playlist. Once all the programming was done, we also made a 3D printed structure to hold the whole music system. Our device, as well as both team members, can be seen in Figure 1 below. </p>
    </div>
    <!--
    <div style="text-align:center;">
            <h2>Introduction</h2>
            <h4 style="text-align: left;text-indent: 0.8cm">Motivation: </h4>
            <p style="text-align: left;padding: 0px 30px;">We are first inspired on how drones are able to track the user as they record a video, providing a convenient experience for the user. We then thought about having a ground robot that has a similar tracking capability, where it will be able to carry things around and also operate indoors. The user will be able to seamlessly interact with the robot using finger gestures. This robot can be installed on shopping carts in grocery stores or luggage carts in the airport. Having this robot will allow the user to conserve their energy (from carrying items or pushing carts) and also focus their attention on other things, such as looking for the things they wanted to get from the grocery racks.</p>
            <h4 style="text-align: left;text-indent: 0.8cm">Solution: </h4>
            <p style="text-align: left;padding: 0px 30px;">A robot is designed to be able to track and follow a ball (representing a human), as well as to read hand-gesture inputs from the user. Computer Vision is utilized to process the video input the camera to detect for the ball and hand gestures. PID controller mechanism is used for the robot motor for smooth tracking.</p>
    </div>
    -->
    

    <hr id='design'>

      <div style="text-align:center;">
              <h2>Design and Testing</h2>
              <p style="text-align: left;padding: 0px 30px;">                
              </p>
                <br>
                <img class="img-rounded" src="" alt="Generic placeholder image" width="600" style="text-align: center">
                <br> A picture of the design
              <p style="text-align: left;padding: 0px 30px;">  
              </p>
                <br>
                <!-- <img class="img-rounded" src="pics/ball-diagram.png" alt="Generic placeholder image" width="600">  -->              
            <p style="text-align: left;padding: 0px 30px;">
                
            
                <br>In the section below, we will have a more in-depth discussion on how each subsystem is designed.
            </p>
              <h4 style="text-align: left;text-indent: 0.8cm">Ball tracking system: </h4>
              <p style="text-align: left;padding: 0px 30px;"> </p>

              <h4 style="text-align: left;text-indent: 0.8cm">Hand gesture detection: </h4>
             <!--
              <h4 style="text-align: left;text-indent: 0.8cm">Motor Control: </h4>
              <p style="text-align: left;padding: 0px 30px;">The robot system was based on the robot base platform used in ECE 5725 labs. The robot base is driven by two DC motors, which are connected to a motor controller. Using the motor controller, we are able to control the speed of the motor using the duty cycle of the PWM signal and to control the motor direction using the direction pin signals. The connection setup of a single motor is shown below.</p>
              <img class="img-rounded" src="pics/circuit1.png" alt="Generic placeholder image" width="600" style="text-align: center">
              <br>Schematic diagram of a single motor setup.<br><br>
              <p style="text-align: left;padding: 0px 30px;">During the manual forward and backward movement, the robot will have a constant duty cycle and direction throughout the process. On the other hand, during the ball tracking mode, we are setting the individual motor speed depending of the position of the tracked ball. We decided to use a PID control algorithm to calculate the duty cycle of the PWM signal for the each motor. This would allow a smooth steering of the robot in tracking the ball.</p>
              <img class="img-rounded" src="pics/pid.jpeg" alt="Generic placeholder image" width="600" style="text-align: center">
              <br>Flowchart of a PID control algorithm.<br><br>
              <p style="text-align: left;padding: 0px 30px;">We have decided to only use PD control considering that our robot was a relatively simple system. The integral control would add complexity to our program and could possibly cause problems more than improve the performance of our robot. In the ball detection program, the distance from the edge of the ball to the edge of the screen was calculated for both sides of the ball. The PD control used these two values as the inputs. Two separate functions were used to calculate the speed of two motors. The proportional term of the motor speed was calculated by the distance times the proportional coefficient (kp), where the motor will be faster if the ball is further, and vice versa. The derivative term was calculated by the current distance minus the distance in the previous frame then times the derivative coefficient (kd), which allows some level of smoothing for velocity changes.</p>
              <p style="text-align: center;padding: 0px 30px;">speed = kp * distance + kd * (distance - previous_distance)</p>

              <h4 style="text-align: left;text-indent: 0.8cm">User Interface: </h4>
              <p style="text-align: left;padding: 0px 30px;">The User Interface will be displayed on the touchscreen PiTFT display attached to the Raspberry Pi. This will be done using the PyGame library, which allows us to display text or buttons on the display.
                The display will be rather simple with a goal of an "easy-to-use" robot. During the "Stop" mode, the screen will display a green button to go to the "Ball-Tracking" mode. It will also display the current robot mode mode. During any other mode where the robot will be moving, the screen will display the red button to go to "Stop" mode. During the "Ball-Tracking" mode, aside from displaying the red button, it will also display how far the detected ball is from the robot in centimeters. </p> -->
      </div>
    <!--
    <hr id='testings'>

      <div style="text-align:center;">
              <h2>Testings</h2>
              <h4 style="text-align: left;text-indent: 0.8cm">Ball tracking system: </h4>
              <p style="text-align: left;padding: 0px 30px;">
              </p>

              <h4 style="text-align: left;text-indent: 0.8cm">Hand gesture detection: </h4>
              <p style="text-align: left;padding: 0px 30px;"> </p>

      </div>
      <div style="text-align:left;">        
        <pre><code>
            inser Generic code</code></pre>
      </div>
      <div style="text-align:center;">
              <p style="text-align: left;padding: 0px 30px;">
              </p>
              <h4 style="text-align: left;text-indent: 0.8cm">Motor Control: </h4>
              <p style="text-align: left;padding: 0px 30px;"></p>
              <br> Several images describing how our robot system was tested.<br><br>
        </div>
        <div style="text-align:left;">        
                <pre><code>
                # genereic code</code></pre>
            
            
            <h4 style="text-align: left;text-indent: 0.8cm">User Interface: </h4>
            <p style="text-align: left;padding: 0px 30px;"></p>
            
            
      </div>
      <div style="text-align: center">
        <img class="img-rounded" src="" alt="Generic placeholder image" style="width:30%;">
        <img class="img-rounded" src="" alt="Generic placeholder image" style="width:30%;">
    </div> -->
      
      
            

    <hr id='results'>

      <div style="text-align:center;">
              <h2>Results</h2>
              <p style="text-align: left;padding: 0px 30px;"></p>

      </div>
    
    
      <hr id='future'>

      <div style="text-align:center;">
              <h2>Future Works</h2>
              <p style="text-align: left;padding: 0px 30px;"> </p>
      </div>

      <hr id='conclusion'>

      <div style="text-align:center;">
              <h2>Conclusion</h2>
      </div>

    <hr>

    <div style="font-size:18px; text-align: center">
      <h2>Parts List</h2>
      <ul style="text-align: left; text-indent: 10cm">
          <p>- Raspberry Pi 4 $35.00</p>
          <a href="https://www.adafruit.com/product/1601"><p>- Adafruit PiTFT $35.00</p></a>
          <a href="https://www.raspberrypi.org/products/camera-module-v2/"><p>- Raspberry Pi Camera Module V2 $25.00</p></a>
          <p>- Robot base with DC motors $25.00</p>
          <p>- LEDs, Resistors and Wires - Provided during the course</p>
      </ul>
      <h4 style="text-align: center;">Total: $95.00</h4>
  </div>

    

    <hr>
    
    <div class="row" style="text-align:center;">
      <h2>Work Distribution</h2>
      <div class="col-md-6" style="font-size:16px">
          <img class="img-rounded" src="" alt="Generic placeholder image" width="240" height="240">
          <h3>Tamzid Ahmed</h3>
          <p class="lead">ta326@cornell.edu<br>ECE '21</p>
          <div style="text-align:left">
            <li>some work
          </div>
      </div>
      <div class="col-md-6" style="font-size:16px">
          <img class="img-rounded" src="" alt="Generic placeholder image" width="240" height="240">
          <h3>Claire Caplan</h3>
          <p class="lead">crc236@cornell.edu<br>ECE '21</p>
          <div style="text-align:left">
                <li>
          </div>
      </div>
  </div>

      <hr>
      <div style="font-size:18px">
          <h2>References</h2>
          <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
          <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
          <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>
          <a href="http://creat-tabu.blogspot.com/2013/08/opencv-python-hand-gesture-recognition.html">Hand gesture computer vision tutorial</a><br>
          <a href="https://courses.ece.cornell.edu/ece5990/ECE5725_Spring2020_Projects/May_15_Demo/Air%20Painter/awt46_sc2524_Thursday-3/awt46_sc2524_Thursday/index.html">ECE 5725 Air Painter project</a><br>
          <a href="https://courses.ece.cornell.edu/ece5990/ECE5725_Fall2018_Projects/ty359_yw996_Final_Project/index.html">ECE 5725 Tracking Robotic Car project</a><br>

      </div>

    <hr>

      <div class="row" style="font-size:18px">
              <h2>Code Appendix</h2>
              
              <a href="https://github.com/JonathanNusantara/BallTrackingRobot">Github Repository</a><br><br>
              <pre><code>
              </code></pre>
      </div>

    </div><!-- /.container -->




    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>
